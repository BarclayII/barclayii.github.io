<!DOCTYPE html>
<html lang="en">
<head>

        <title>Authorship Attribution and Topic Discovery</title>
        <meta charset="utf-8" />


        <!-- Mobile viewport optimized: j.mp/bplateviewport -->
        <meta name="viewport" content="width=device-width,initial-scale=1, maximum-scale=1">

        <link rel="stylesheet" type="text/css" href="/theme/gumby.css" />
        <link rel="stylesheet" type="text/css" href="/theme/style.css" />
        <link rel="stylesheet" type="text/css" href="/theme/pygment.css" />

        <script src="/theme/js/libs/modernizr-2.6.2.min.js"></script>




</head>

<body id="index" class="home">


    <div class="container">

        <div class="row">

          <header id="banner" class="body">
                  <h1><a href="/">Quan Gan (Andy) @ NYU <strong></strong></a></h1>
          </header><!-- /#banner -->

            <div id="navigation" class="navbar row">
              <a href="#" gumby-trigger="#navigation &gt; ul" class="toggle"><i class="icon-menu"></i></a>
             
              <ul class="columns">
                <li><a href="/">Home</a></li>


              </ul>
            </div>

<section id="content" class="body">

   <div class="row">
        <div class="eleven columns">


            <header>
              <h2 class="entry-title">
                <a href="/authorship-attribution-and-topic-discovery.html" rel="bookmark"
                   title="Permalink to Authorship Attribution and Topic Discovery">Authorship Attribution and Topic Discovery</a></h2>
           
            </header>
            <footer class="post-info">
              <abbr class="published" title="2017-11-27T03:28:10.449248-05:00">
                Mon 27 November 2017
              </abbr>
              <address class="vcard author">By 
                <a class="url fn" href="/author/quan-gan-andy.html"> Quan Gan (Andy)</a>
              </address>
            </footer><!-- /.post-info -->
            <div class="entry-content">
              <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

<p>As a project for <a href="https://github.com/joanbruna/ir17">Inference and Representation</a> course in NYU, me and <a href="https://github.com/BruceChaun">this guy</a> decided to do some modeling of academic author interests.  Such model would have several practical applications:</p>
<ul>
<li>Recommending latest papers to researchers, like what <a href="https://www.mendeley.com">Mendeley</a> and <a href="https://academic.microsoft.com">Microsoft Academic (MSA)</a> does.</li>
<li>Recommending the researchers to each other on some social network.  Both Mendeley and MSA have this.  I wonder why not a lot of people is using it as a social platform...</li>
<li>Automatically assigning papers to reviewers.  Actually we already have <a href="http://torontopapermatching.org/webapp/profileBrowser/about_us/">Toronto Paper Matching System (TPMS)</a> adopted in multiple conference review systems.</li>
</ul>
<p>Nonetheless, I have to admit that the main motivation (at least for me) of doing this project is to enable some ways to visualize the authors as groups, and inspect them with my own eyes.  After all, it should be a lot of fun to see which researchers are close to each other on a figure.</p>
<p><strong>The code is available <a href="https://github.com/BruceChaun/ArxivInference">here</a>.</strong></p>
<h4>Dataset</h4>
<p>Preferably, we would like a dataset that has strong indicator of "interest".  A good example would be the user database of Mendeley or MSA, where actual users have an archive of papers; such papers obviously are of the user's interest.  Another good dataset would be those from TPMS.</p>
<p>Unfortunately, it is unlikely for MSA and Mendeley to share their database to the public (of course).  Also the dataset used by TPMS is not available.  Therefore, we have to use <em>authorship</em> as a proxy of author interest.  After all, a person should be sufficiently interested on some topic so that he/she can actually write something about it.</p>
<p>It is much easier to grab authorship data, since there are already a number of online libraries, such as <a href="https://www.arxiv.org">arXiv</a> and MSA.  Both libraries have bulk data access Web API where one can easily scrape metadata of all the papers, including the authors, abstracts, titles, etc.</p>
<ul>
<li>We have a NIPS papers dataset, where <a href="https://mimno.infosci.cornell.edu/info6150/readings/398.pdf">Author Topic Model</a> and <a href="https://www.cl.cam.ac.uk/~np394/docs/hdm_2014_paper.pdf">the supervised version</a> were both evaluated.</li>
<li>Thanks to Andrej Karpathy's <a href="https://www.arxiv-sanity.com">arxiv-sanity</a>, we can conveniently grab thousands of machine learning research papers.  We crawled 49,980 papers using arXiv-sanity, covering the research from 1997 to 2017 (a 20-year span).</li>
</ul>
<p>For all datasets, we lowercase all the words, and removed the stop words.</p>
<h4>Model</h4>
<p>The model we were using is very simple:</p>
<ol>
<li>We represent each word and each author as 20-dimension embeddings, and compute a score of authorship by dot product between the author embedding and the average word embedding of a document.</li>
<li>We minimize a pairwise ranking loss with negative sampling on both authors and documents.</li>
</ol>
<p>More formally, we have a dataset of <span><span class="MathJax_Preview">$D$</span><script type="math/tex">D</script></span> papers, where each document has a list of words <span><span class="MathJax_Preview">$\mathcal{D}_i = (w_{i_1}, ..., w_{i_{n_i}})$</span><script type="math/tex">\mathcal{D}_i = (w_{i_1}, ..., w_{i_{n_i}})</script></span> and a list of authors <span><span class="MathJax_Preview">$\mathcal{A}_i = (a_{i_1}, ..., a_{i_{m_i}})$</span><script type="math/tex">\mathcal{A}_i = (a_{i_1}, ..., a_{i_{m_i}})</script></span>.  The total number of authors in the dataset is <span><span class="MathJax_Preview">$A$</span><script type="math/tex">A</script></span>.</p>
<p>We first convert all <span><span class="MathJax_Preview">$w_{i,j}$</span><script type="math/tex">w_{i,j}</script></span> and <span><span class="MathJax_Preview">$a_{i,k}$</span><script type="math/tex">a_{i,k}</script></span> to some embedding vectors <span><span class="MathJax_Preview">$\mathbf{w}_{i,j}$</span><script type="math/tex">\mathbf{w}_{i,j}</script></span> and <span><span class="MathJax_Preview">$\mathbf{a}_{i,k}$</span><script type="math/tex">\mathbf{a}_{i,k}</script></span>, using embedding matrices <span><span class="MathJax_Preview">$\mathbf{W}$</span><script type="math/tex">\mathbf{W}</script></span> and <span><span class="MathJax_Preview">$\mathbf{A}$</span><script type="math/tex">\mathbf{A}</script></span> for words and authors respectively.  Then we compute the bag-of-words embedding and a bag-of-authors embedding:</p>
<p>
<div><span class="MathJax_Preview">$$
\bar{\mathbf{w}}_i = \dfrac{1}{n_i}\sum_{j=1}^{n_i}\mathbf{w}_{i,j} \\
\bar{\mathbf{a}}_i = \dfrac{1}{m_i}\sum_{k=1}^{m_i}\mathbf{a}_{i,k}
$$</span><script type="math/tex; mode=display">
\bar{\mathbf{w}}_i = \dfrac{1}{n_i}\sum_{j=1}^{n_i}\mathbf{w}_{i,j} \\
\bar{\mathbf{a}}_i = \dfrac{1}{m_i}\sum_{k=1}^{m_i}\mathbf{a}_{i,k}
</script>
</div>
</p>
<p>The score is simply <span><span class="MathJax_Preview">$s_i = \bar{\mathbf{w}}_i^T\bar{\mathbf{a}}_i$</span><script type="math/tex">s_i = \bar{\mathbf{w}}_i^T\bar{\mathbf{a}}_i</script></span>.</p>
<p>We chose ranking loss only because we don't feel like assigning a concrete number as a target for optimization.  Rather, we think it makes more sense to just tell the model to "prefer" some authors/documents than others.  In order to use ranking loss, we need some negative samples:</p>
<ul>
<li>For document, we simply sample one uniformly in the corpus and we can similarly have a bag-of-word <span><span class="MathJax_Preview">$\bar{\mathbf{w}}_i'$</span><script type="math/tex">\bar{\mathbf{w}}_i'</script></span>.</li>
<li>For authors, we treat the set of all authors, excluding <span><span class="MathJax_Preview">$\mathcal{A}_i$</span><script type="math/tex">\mathcal{A}_i</script></span>, as the negative sample of authors.  The formulation is simply
  <div><span class="MathJax_Preview">$$
  \bar{\mathbf{a}}_i'=\dfrac{1}{A-m_i}\left(\sum_{k=1}^A\mathbf{a}_{i,k} - m_i \bar{\mathbf{a}}_i\right)
  $$</span><script type="math/tex; mode=display">
  \bar{\mathbf{a}}_i'=\dfrac{1}{A-m_i}\left(\sum_{k=1}^A\mathbf{a}_{i,k} - m_i \bar{\mathbf{a}}_i\right)
  </script>
</div>
</li>
</ul>
<p>The loss function we are going to minimize is then simply two pairwise ranking losses, one for negative author samples and another for negative document samples, plus regularization terms:</p>
<p>
<div><span class="MathJax_Preview">$$
\mathcal{L}_i = \max(0, 1 + \bar{\mathbf{w}}_i'^T \bar{\mathbf{a}}_i -
\bar{\mathbf{w}}_i^T \bar{\mathbf{a}}_i) +
\max(0, 1 + \bar{\mathbf{w}}_i^T \bar{\mathbf{a}}_i' -
\bar{\mathbf{w}}_i^T \bar{\mathbf{a}}_i) \\
\mathcal{L} = \dfrac{1}{D} \sum_{i=1}^D \mathcal{L}_i + \lambda_A \lVert \mathbf{A} \rVert_2^2 + \lambda_W \lVert \mathbf{W} \rVert_2^2
$$</span><script type="math/tex; mode=display">
\mathcal{L}_i = \max(0, 1 + \bar{\mathbf{w}}_i'^T \bar{\mathbf{a}}_i -
\bar{\mathbf{w}}_i^T \bar{\mathbf{a}}_i) +
\max(0, 1 + \bar{\mathbf{w}}_i^T \bar{\mathbf{a}}_i' -
\bar{\mathbf{w}}_i^T \bar{\mathbf{a}}_i) \\
\mathcal{L} = \dfrac{1}{D} \sum_{i=1}^D \mathcal{L}_i + \lambda_A \lVert \mathbf{A} \rVert_2^2 + \lambda_W \lVert \mathbf{W} \rVert_2^2
</script>
</div>
</p>
<p>where <span><span class="MathJax_Preview">$\lambda_A = \lambda_W = 1\times 10^{-5}$</span><script type="math/tex">\lambda_A = \lambda_W = 1\times 10^{-5}</script></span>.</p>
<p>We implemented the model in PyTorch.  It's trainable on CPU, albeit very slowly.  We believe that if we tailor the implementation in C or Fortran it will become a lot faster.  Also, note that if we fix <span><span class="MathJax_Preview">$\mathbf{W}$</span><script type="math/tex">\mathbf{W}</script></span>, the loss function would be convex w.r.t. <span><span class="MathJax_Preview">$\mathbf{A}$</span><script type="math/tex">\mathbf{A}</script></span>, and vice versa.  Not sure if doing alternating (stochastic) convex optimization would speed up.</p>
<h4>Quantitative Results</h4>
<ol>
<li>
<p>We evaluated our model on the NIPS 1987-2013 dataset, same as what the Supervised Author-Topic Model did.  We got our own set of papers from <a href="https://www.kaggle.com/benhamner/nips-papers">here</a>.  The resulting dataset is different from the original paper though: we have 2372 papers, 2577 authors and a vocabulary of 181948 words; the author set and the vocabulary is larger than theirs.  Not sure if it helps the performance, but our model seems to be significantly better in terms of AUC on multiple authors:
<table class="table-hover">
<thead>
<tr>
<th style="text-align:center">Model</th>
<th style="text-align:center">AUC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Supervised Author-Topic Model</td>
<td style="text-align:center">0.67</td>
</tr>
<tr>
<td style="text-align:center">Author-Topic Model</td>
<td style="text-align:center">0.62</td>
</tr>
<tr>
<td style="text-align:center">RF: Single-tree</td>
<td style="text-align:center">0.55</td>
</tr>
<tr>
<td style="text-align:center">RF: 5-trees</td>
<td style="text-align:center">0.71</td>
</tr>
<tr>
<td style="text-align:center">Ours</td>
<td style="text-align:center">0.85</td>
</tr>
</tbody>
</table></p>
</li>
<li>
<p>We also evaluated the average precision over 1, 5, 10, 50.  The result is pretty lame though:
<table class="table-hover">
<thead>
<tr>
<th style="text-align:center">K</th>
<th style="text-align:center">AP@K</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0.088</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">0.092</td>
</tr>
<tr>
<td style="text-align:center">10</td>
<td style="text-align:center">0.099</td>
</tr>
<tr>
<td style="text-align:center">50</td>
<td style="text-align:center">0.109</td>
</tr>
</tbody>
</table></p>
</li>
<li>For each document-author pair in the dataset, we also computed the ranking of that author among everybody, and took the median.  The result is 101 out of 2577 --- OK'ish, but not great.</li>
</ol>
<h4>Discovering Topics</h4>
<p>We can do a lot of things as qualitative analysis:
<em> Given an author, propose a list of words associated to the topic of his/her interest.  We can do that by ranking all the words <span><span class="MathJax_Preview">$w_j$</span><script type="math/tex">w_j</script></span> with the scores <span><span class="MathJax_Preview">$\mathbf{w}_j^T \mathbf{a}_i$</span><script type="math/tex">\mathbf{w}_j^T \mathbf{a}_i</script></span>.
</em> Given an author, find his/her nearest neighbors (or visualize the embeddings using t-SNE).</p>
<p>We evaluate qualitatively using the arXiv dataset.  We first divided the dataset into training, validation, and test by 8:1:1.  Then, we only keep the authors which appeared at least 3 times in the training set as the author set.  We finally discard every paper in all three partitions where none of the authors is in the author set, and only kept the paper abstracts as documents.  Consequently, we have:</p>
<ul>
<li>a vocabulary of 91719 words</li>
<li>a set of 11492 authors</li>
<li>a training set of 31923 papers</li>
<li>a validation set of 3577 papers</li>
<li>a test set of 3521 papers</li>
</ul>
<p>Here is the top-ranked words given an author.  Here I picked some authors that I have heard of, and grouped them into several categories.  Yes, it means that I'm the ~~noisy~~ classifier of author interests.</p>
<p><strong>I'm also considering releasing a web-app demo but I don't know if I have time to do it.</strong></p>
<p>The meaning of <span><span class="MathJax_Preview">$\lambda_c$</span><script type="math/tex">\lambda_c</script></span> is explained in the following section.</p>
<table class="table-hover">
<thead>
<tr>
<th style="text-align:center">Name</th>
<th style="text-align:center">Top-5 words</th>
<th style="text-align:center">Top-5 words with <span class="mathjax"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4871-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BB;</mi><mi>c</mi></msub><mo>=</mo><mn>0.1</mn></math>" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-61892" style="width: 4.306em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.537em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1003.49em 2.513em -999.997em); top: -2.2em; left: 0em;"><span class="mrow" id="MathJax-Span-61893"><span class="msubsup" id="MathJax-Span-61894"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.152em -999.997em); top: -3.993em; left: 0em;"><span class="mi" id="MathJax-Span-61895" style="font-family: MathJax_Math; font-style: italic;">λ</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.566em;"><span class="mi" id="MathJax-Span-61896" style="font-size: 70.7%; font-family: MathJax_Math; font-style: italic;">c</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-61897" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="mn" id="MathJax-Span-61898" style="font-family: MathJax_Main; padding-left: 0.259em;">0.1</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>λ</mi><mi>c</mi></msub><mo>=</mo><mn>0.1</mn></math></span></span><script type="math/tex" id="MathJax-Element-4871">\lambda_c=0.1</script></span></th>
<th style="text-align:center">Top-5 words with <span class="mathjax"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4872-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BB;</mi><mi>c</mi></msub><mo>=</mo><mn>1</mn></math>" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-61899" style="width: 3.384em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.769em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1002.72em 2.513em -999.997em); top: -2.2em; left: 0em;"><span class="mrow" id="MathJax-Span-61900"><span class="msubsup" id="MathJax-Span-61901"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.152em -999.997em); top: -3.993em; left: 0em;"><span class="mi" id="MathJax-Span-61902" style="font-family: MathJax_Math; font-style: italic;">λ</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.566em;"><span class="mi" id="MathJax-Span-61903" style="font-size: 70.7%; font-family: MathJax_Math; font-style: italic;">c</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span><span class="mo" id="MathJax-Span-61904" style="font-family: MathJax_Main; padding-left: 0.259em;">=</span><span class="mn" id="MathJax-Span-61905" style="font-family: MathJax_Main; padding-left: 0.259em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>λ</mi><mi>c</mi></msub><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-4872">\lambda_c=1</script></span></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Li Fei-Fei</td>
<td style="text-align:center"><code>visual images image video object</code></td>
<td style="text-align:center"><code>visual video object image images</code></td>
<td style="text-align:center"><code>visual video object deep saliency</code></td>
</tr>
<tr>
<td style="text-align:center">Kaiming He</td>
<td style="text-align:center"><code>image images networks neural deep</code></td>
<td style="text-align:center"><code>deep image convolutional images visual</code></td>
<td style="text-align:center"><code>video image visual 3d saliency</code></td>
</tr>
<tr>
<td style="text-align:center">Seunghoon Hong</td>
<td style="text-align:center"><code>images proposed visual video object</code></td>
<td style="text-align:center"><code>image images video segmentation visual</code></td>
<td style="text-align:center"><code>video saliency 3d segmentation image</code></td>
</tr>
<tr>
<td style="text-align:center">Rob Fergus</td>
<td style="text-align:center"><code>model models image images visual</code></td>
<td style="text-align:center"><code>deep visual training neural object</code></td>
<td style="text-align:center"><code>adversarial deep visual dropout video</code></td>
</tr>
<tr>
<td style="text-align:center">Kyunghyun Cho</td>
<td style="text-align:center"><code>neural model language network word</code></td>
<td style="text-align:center"><code>neural speech training deep word</code></td>
<td style="text-align:center"><code>speech word rnn lstm sentiment</code></td>
</tr>
<tr>
<td style="text-align:center">Chris Dyer</td>
<td style="text-align:center"><code>models language show word training</code></td>
<td style="text-align:center"><code>language word text words translation</code></td>
<td style="text-align:center"><code>word entity embedding translation sentiment</code></td>
</tr>
<tr>
<td style="text-align:center">Tomas Mikolov</td>
<td style="text-align:center"><code>learning language word machine task</code></td>
<td style="text-align:center"><code>language word text translation words</code></td>
<td style="text-align:center"><code>word dialogue sentiment translation language</code></td>
</tr>
<tr>
<td style="text-align:center">Samuel R. Bowman</td>
<td style="text-align:center"><code>model models neural language networks</code></td>
<td style="text-align:center"><code>language word translation models text</code></td>
<td style="text-align:center"><code>word translation dialogue speech language</code></td>
</tr>
<tr>
<td style="text-align:center">Sergey Levine</td>
<td style="text-align:center"><code>learning approach training policy using</code></td>
<td style="text-align:center"><code>policy learning reinforcement robot agents</code></td>
<td style="text-align:center"><code>robot policy rl reinforcement reward</code></td>
</tr>
<tr>
<td style="text-align:center">David Silver</td>
<td style="text-align:center"><code>learning algorithm policy algorithms problem</code></td>
<td style="text-align:center"><code>policy agents reinforcement learning game</code></td>
<td style="text-align:center"><code>policy agents rl reinforcement planning</code></td>
</tr>
<tr>
<td style="text-align:center">Volodymyr Mnih</td>
<td style="text-align:center"><code>learning neural networks network deep</code></td>
<td style="text-align:center"><code>neural deep networks reinforcement agents</code></td>
<td style="text-align:center"><code>rl policy reinforcement robot agent</code></td>
</tr>
<tr>
<td style="text-align:center">Pieter Abbeel</td>
<td style="text-align:center"><code>learning approach show policy training</code></td>
<td style="text-align:center"><code>learning policy reinforcement agents robot</code></td>
<td style="text-align:center"><code>policy rl robot reinforcement reward</code></td>
</tr>
<tr>
<td style="text-align:center">David M. Blei</td>
<td style="text-align:center"><code>models model inference data bayesian</code></td>
<td style="text-align:center"><code>inference bayesian models model latent</code></td>
<td style="text-align:center"><code>causal bayesian inference latent variational</code></td>
</tr>
<tr>
<td style="text-align:center">Daphne Koller</td>
<td style="text-align:center"><code>models inference bayesian model approach</code></td>
<td style="text-align:center"><code>bayesian inference probabilistic uncertainty causal</code></td>
<td style="text-align:center"><code>causal policy bayesian agents quantum</code></td>
</tr>
<tr>
<td style="text-align:center">David Sontag</td>
<td style="text-align:center"><code>models data model inference algorithm</code></td>
<td style="text-align:center"><code>inference models bayesian latent model</code></td>
<td style="text-align:center"><code>latent ml variational inference predictive</code></td>
</tr>
<tr>
<td style="text-align:center">Mehryar Mohri</td>
<td style="text-align:center"><code>algorithm problem algorithms learning show</code></td>
<td style="text-align:center"><code>bounds algorithms algorithm optimization bound</code></td>
<td style="text-align:center"><code>regret bounds fairness causal bound</code></td>
</tr>
<tr>
<td style="text-align:center">Joan Bruna</td>
<td style="text-align:center"><code>networks network neural model deep</code></td>
<td style="text-align:center"><code>networks network deep generative neural</code></td>
<td style="text-align:center"><code>dropout kernel adversarial gans generative</code></td>
</tr>
<tr>
<td style="text-align:center">Ian Goodfellow</td>
<td style="text-align:center"><code>learning neural training models deep</code></td>
<td style="text-align:center"><code>learning neural deep adversarial training</code></td>
<td style="text-align:center"><code>adversarial dropout gans generative gan</code></td>
</tr>
<tr>
<td style="text-align:center">Nicolas Papernot</td>
<td style="text-align:center"><code>learning training data neural machine</code></td>
<td style="text-align:center"><code>learning adversarial deep training machine</code></td>
<td style="text-align:center"><code>adversarial dropout neurons gans genertive</code></td>
</tr>
<tr>
<td style="text-align:center">Yann LeCun</td>
<td style="text-align:center"><code>training networks deep learning show</code></td>
<td style="text-align:center"><code>deep training networks neural adversarial</code></td>
<td style="text-align:center"><code>adversarial dropout deep gans gan</code></td>
</tr>
<tr>
<td style="text-align:center">Yoshua Bengio</td>
<td style="text-align:center"><code>learning neural training networks models</code></td>
<td style="text-align:center"><code>neural networks deep training learning</code></td>
<td style="text-align:center"><code>adversarial dropout generative gans neurons</code></td>
</tr>
<tr>
<td style="text-align:center">Geoffrey E. Hinton</td>
<td style="text-align:center"><code>data models learning model training</code></td>
<td style="text-align:center"><code>models learning model neural training</code></td>
<td style="text-align:center"><code>adversarial dropout generative gans rnn</code></td>
</tr>
</tbody>
</table>

<p>It seems that other than the computer vision folks, the model tend to assign more generic words that can be applied to <em>anybody</em> (such as <code>model</code>, <code>learning</code>, etc.) as top-ranked words.  This issue is more severe on the highest-ranked words: those words simply didn't tell us anything about the author's interest.</p>
<h4>Inverse Document Frequency Regularizer?</h4>
<p>Presumably, common words like <code>model</code>, <code>learning</code> should appear across a majority of the papers <em>regardless</em> of the actual topics.  Since we have L2 regularization on the word embeddings, words that appear more often would have a larger L2 norm than the rare ones.  And because we are computing the scores by dot products, the embeddings that have a bigger L2 norm will dominate the smaller ones.  For instance, in the model above, the embedding for <code>learning</code> has an L2 norm of 23.06, while the one for <code>policy</code> is only 5.68.</p>
<p>Penalizing the norm of the more frequent words should help.  However, we probably don't want to regularize a word that occurs frequently in a <em>single</em> document but nowhere else; such word is very likely informative.  Therefore, we only want to penalize the word based on <em>the number of documents where the word showed up</em>.  We can regard this idea as an analogy to inverse document frequency (IDF).</p>
<p>This regularization loss can be written as the following:
<div><span class="MathJax_Preview">$$
\mathcal{L}_c = \lambda_c \sum_{i=1}^V C_i \lVert \mathbf{w}_i \rVert_2^2
$$</span><script type="math/tex; mode=display">
\mathcal{L}_c = \lambda_c \sum_{i=1}^V C_i \lVert \mathbf{w}_i \rVert_2^2
</script>
</div>
where <span><span class="MathJax_Preview">$C_i$</span><script type="math/tex">C_i</script></span> is the number of documents the word <span><span class="MathJax_Preview">$w_i$</span><script type="math/tex">w_i</script></span> showed up.  In stochastic gradient descent, <span><span class="MathJax_Preview">$C_i$</span><script type="math/tex">C_i</script></span> is simply counted from the documents in the minibatch.</p>
<p>We evaluate the effect of this regularization both quantitatively and qualitatively.  For comparison, we counted the number of different words occurred in top-<span><span class="MathJax_Preview">$K$</span><script type="math/tex">K</script></span> ranking, and we compute the entropy of the normalized word occurrences.  Those having a more diverse list of proposed words should have a higher entropy.  We also give the median of author ranking and average precision to demonstrate how this regularization hurts our model:</p>
<table class="table-hover">
<thead>
<tr>
<th style="text-align:center"><span class="mathjax"><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3431-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>&amp;#x03BB;</mi><mi>c</mi></msub></math>" role="presentation" style="position: relative;"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-53008" style="width: 1.181em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px; font-size: 122%;"><span style="position: absolute; clip: rect(1.335em 1000.98em 2.513em -999.997em); top: -2.2em; left: 0em;"><span class="mrow" id="MathJax-Span-53009"><span class="msubsup" id="MathJax-Span-53010"><span style="display: inline-block; position: relative; width: 0.976em; height: 0px;"><span style="position: absolute; clip: rect(3.128em 1000.57em 4.152em -999.997em); top: -3.993em; left: 0em;"><span class="mi" id="MathJax-Span-53011" style="font-family: MathJax_Math; font-style: italic;">λ</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.566em;"><span class="mi" id="MathJax-Span-53012" style="font-size: 70.7%; font-family: MathJax_Math; font-style: italic;">c</span><span style="display: inline-block; width: 0px; height: 3.998em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.205em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.247em; border-left: 0px solid; width: 0px; height: 1.191em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>λ</mi><mi>c</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-3431">\lambda_c</script></span></th>
<th style="text-align:center">0</th>
<th style="text-align:center">0.1</th>
<th style="text-align:center">1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"># of different words in top-1</td>
<td style="text-align:center">76</td>
<td style="text-align:center">101</td>
<td style="text-align:center">84</td>
</tr>
<tr>
<td style="text-align:center"># of different words in top-5</td>
<td style="text-align:center">159</td>
<td style="text-align:center">221</td>
<td style="text-align:center">207</td>
</tr>
<tr>
<td style="text-align:center"># of different words in top-10</td>
<td style="text-align:center">234</td>
<td style="text-align:center">332</td>
<td style="text-align:center">314</td>
</tr>
<tr>
<td style="text-align:center">Entropy of top-1 word distribution</td>
<td style="text-align:center">2.95</td>
<td style="text-align:center">3.34</td>
<td style="text-align:center">3.45</td>
</tr>
<tr>
<td style="text-align:center">Entropy of top-5 word distribution</td>
<td style="text-align:center">4.04</td>
<td style="text-align:center">4.38</td>
<td style="text-align:center">4.34</td>
</tr>
<tr>
<td style="text-align:center">Entropy of top-10 word distribution</td>
<td style="text-align:center">4.55</td>
<td style="text-align:center">4.88</td>
<td style="text-align:center">4.81</td>
</tr>
<tr>
<td style="text-align:center">AP@1</td>
<td style="text-align:center">0.027</td>
<td style="text-align:center">0.023</td>
<td style="text-align:center">0.017</td>
</tr>
<tr>
<td style="text-align:center">AP@5</td>
<td style="text-align:center">0.020</td>
<td style="text-align:center">0.018</td>
<td style="text-align:center">0.012</td>
</tr>
<tr>
<td style="text-align:center">AP@10</td>
<td style="text-align:center">0.023</td>
<td style="text-align:center">0.021</td>
<td style="text-align:center">0.014</td>
</tr>
<tr>
<td style="text-align:center">AP@50</td>
<td style="text-align:center">0.028</td>
<td style="text-align:center">0.026</td>
<td style="text-align:center">0.017</td>
</tr>
<tr>
<td style="text-align:center">Median of ranking</td>
<td style="text-align:center">544</td>
<td style="text-align:center">548</td>
<td style="text-align:center">727</td>
</tr>
</tbody>
</table>

<p>The top-ranked words for the authors with non-zero <span><span class="MathJax_Preview">$\lambda_c$</span><script type="math/tex">\lambda_c</script></span> is shown above.  We can see that when <span><span class="MathJax_Preview">$\lambda_c$</span><script type="math/tex">\lambda_c</script></span> is too big, rare words did occur more frequently --- but the model also fills rare words in random places (e.g. <code>adversarial</code> and <code>gan</code> seems to be all over the place).</p>
<h4>Embedding visualizations</h4>
<p>I tabulated the embeddings of authors which occurred more than 5 times:</p>
<ul>
<li><a href="https://raw.githubusercontent.com/BruceChaun/ArxivInference/master/users.tsv">Metadata file</a></li>
<li><a href="https://raw.githubusercontent.com/BruceChaun/ArxivInference/master/embedding.tsv">Embedding</a><ul>
<li><a href="http://projector.tensorflow.org/?config=https://raw.githubusercontent.com/BruceChaun/ArxivInference/master/projector.json">See on Tensorflow Projector</a></li>
</ul>
</li>
<li><a href="https://raw.githubusercontent.com/BruceChaun/ArxivInference/master/embedding-lambda%3D0.1.tsv">Embedding with <span><span class="MathJax_Preview">$\lambda_c=0.1$</span><script type="math/tex">\lambda_c=0.1</script></span></a><ul>
<li><a href="http://projector.tensorflow.org/?config=https://raw.githubusercontent.com/BruceChaun/ArxivInference/master/projector-lambda%3D0.1.json">See on Tensorflow Projector</a></li>
</ul>
</li>
</ul>
            </div><!-- /.entry-content -->


        </div><!-- /.eleven.columns -->

<div class="three columns">

<h4>Pages</h4>

 <ul>
  </ul>

<h4>Categories</h4>
<ul class="blank">
		<li><a href="/category/misc.html">misc</a></li>
		<li><a href="/category/nlp.html">nlp</a></li>
</ul>


<h4>Tags</h4>
	<ul class="blank">
</ul>



</div> </div><!-- /.row -->


</section>

       </div><!-- /.row -->
    </div><!-- /.container -->


       <div class="container.nopad bg">

    
        <footer id="credits" class="row">
          <div class="seven columns left-center">

                   <address id="about" class="vcard body">
                    Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                    which takes great advantage of <a href="http://python.org">Python</a>.
                    <br />
                    Based on the <a target="_blank" href="http://gumbyframework.com">Gumby Framework</a>
                    </address>
          </div>


          <div class="seven columns">
            <div class="row">
              <ul class="socbtns">





              </ul>
            </div>
          </div>
        </footer>

    </div>


  <script src="/theme/js/libs/jquery-1.9.1.min.js"></script>
  <script src="/theme/js/libs/gumby.min.js"></script>
  <script src="/theme/js/plugins.js"></script>
</body>
</html>